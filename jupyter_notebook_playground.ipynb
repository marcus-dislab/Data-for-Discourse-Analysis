{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bdb9dd",
   "metadata": {},
   "source": [
    "## Python basics\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basis: variable assignment\n",
    "corpus = \"This is a very tiny corpus. But at least this tiny corpus has a second sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef160a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content of a variable\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in jupyter notebooks that's enough to print a variable - if it's the last row of a cell\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of any Python object (everything is an object in Python)\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec00201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects / data types have their own methods\n",
    "tokens = corpus.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split() returns a list (as indicated by the square brackets)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable/sequential data types support selection and operations based on index positions\n",
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations can be nested and/or concatenated\n",
    "[tokens[0], type(tokens[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings are iterable too\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the \"replace\" method to improve our tokenization on whitespace\n",
    "corpus = corpus.replace(\".\", \" .\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the punctuation gets splitted correctly\n",
    "tokens = corpus.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the built-in method len() calculates the length of iterable data types\n",
    "token_count = len(tokens)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b721e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_count is an integer object\n",
    "type(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86376e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know that our corpus consists of 18 tokens and 82 characters\n",
    "char_count = len(corpus)\n",
    "char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e70014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a for-loop we can access every item in a list or in other iterable data types\n",
    "for token in tokens:\n",
    "    print(\"token:\\t\" + token) # strings can be concatenated\n",
    "    #print(f\"token:\\t{token}\") # the f-string syntax does the trick as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe302a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the sentences of our corpus\n",
    "sentences = corpus.split(\".\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have two tasks: get rid of the empty string and\n",
    "# append the punctuation back to the sentences\n",
    "sentences_stripped = []\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    # with conditions we can check if a statement is true or false\n",
    "    if not sentence == \"\":\n",
    "        sentence += \" .\"\n",
    "        sentences_stripped.append(sentence)\n",
    "\n",
    "sentences_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29031b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts provide a mapping between keys and values\n",
    "token_lengths = {}\n",
    "type(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79dd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can assign values to keys with dict_variable[key] = value\n",
    "for token in tokens:\n",
    "    token_lengths[token] = len(token)\n",
    "\n",
    "token_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a simple counter for the token frequencies of our corpus\n",
    "counter = {}\n",
    "for token in tokens:\n",
    "    # check if we have already seen this token\n",
    "    if token in counter:\n",
    "        counter[token] += 1\n",
    "    # if not (we see this token for the first time):\n",
    "    else:\n",
    "        counter[token] = 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# often programming tasks can be solved in more than one way\n",
    "counter = {}\n",
    "for token in tokens:\n",
    "    if not token in counter:\n",
    "        counter[token] = 0\n",
    "    counter[token] += 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for many problems code already exists, which can be imported\n",
    "# from built-in or external Python modules\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# well, that's shorter...\n",
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40803960",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the most_common() method of Counter\n",
    "# to sort our token frequencies\n",
    "Counter(tokens).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08af4f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## spaCy\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a110f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to import the previously installed library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02050482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a tiny corpus\n",
    "spacy_corpus = \"Mary Lou McDonald grew up in a republican household in Dublin to a backdrop of the Troubles in Northern Ireland. \\\"My family's connections with the IRA would have been in the 1920s,\\\" she says.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beefc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# befor spaCy can annotate anything, we have to load a trained statistical model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f36586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp() runs the default model annotation pipeline on our example corpus\n",
    "# 'doc' now contains the annotated spaCy data object\n",
    "doc = nlp(spacy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the tokens of our corpus document\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy docs are composed of Token objects\n",
    "print(type(doc[0]))\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23780e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token objects have different attributes containing their linguistic annotations\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    print(token.pos_)\n",
    "    print(token.pos_, token.tag_)\n",
    "    print(token.lemma_)\n",
    "    print(token.morph)\n",
    "    print(token.ent_iob_, token.ent_type_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09080ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b21ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88114017",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<text>\\n\"\n",
    "for sentence in doc.sents:\n",
    "    vrt += \"<s>\\n\"\n",
    "    entity = False\n",
    "    for token in sentence:\n",
    "        if entity == False and token.ent_iob_ == \"B\":\n",
    "            entity = True\n",
    "            vrt += f\"<entity type=\\\"{token.ent_type_}\\\">\\n\"\n",
    "        elif entity == True and not token.ent_iob_ == \"I\":\n",
    "            entity = False\n",
    "            vrt += \"</entity type>\\n\"\n",
    "        vrt += \"\\t\".join((token.text, token.pos_, token.lemma_, str(token.morph), token.ent_type_, token.ent_iob_)) + \"\\n\"\n",
    "    vrt += \"</s>\\n\"\n",
    "vrt += \"</text>\\n\"\n",
    "\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921254f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<text>\\n\"\n",
    "for sentence in doc.sents:\n",
    "    vrt += \"<sentence>\\n\"\n",
    "    for token in sentence:\n",
    "        vrt += token.text + \"\\n\"\n",
    "    vrt += \"</sentence>\\n\"\n",
    "vrt += \"</text>\\n\"\n",
    "\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<person>\\nMary\\nLou\\nMcDonald\\n</person>\"\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb873384",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## data formats\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open(\"tweets.jsonl\", encoding=\"utf8\") as jsonl:\n",
    "    for tweet in jsonl:\n",
    "        tweets.append(json.loads(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da88315",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets.csv\", \"w\", encoding=\"utf8\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"author_id\", \"conversation_id\", \"created_at\", \"id\", \"in_reply_to_user_id\", \"like_count\", \"quote_count\", \"reply_count\", \"retweet_count\", \"referenced_tweet_ids\", \"reference_types\", \"text\"]\n",
    "    writer = csv.DictWriter(csvfile, dialect=\"excel\", delimiter=\";\", fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for tweet in tweets:        \n",
    "        tweet_fields = {\n",
    "            \"author_id\": tweet[\"author_id\"],\n",
    "            \"conversation_id\": tweet[\"conversation_id\"],\n",
    "            \"created_at\": tweet[\"created_at\"],\n",
    "            \"id\": tweet[\"id\"],\n",
    "            \"in_reply_to_user_id\": tweet.get(\"in_reply_to_user_id\"),\n",
    "            \"like_count\": tweet.get(\"public_metrics\", {}).get(\"like_count\"),\n",
    "            \"quote_count\": tweet.get(\"public_metrics\", {}).get(\"quote_count\"),\n",
    "            \"reply_count\": tweet.get(\"public_metrics\", {}).get(\"reply_count\"),\n",
    "            \"retweet_count\": tweet.get(\"public_metrics\", {}).get(\"retweet_count\"),\n",
    "            \"referenced_tweet_ids\": \", \".join([referenced.get(\"id\") for referenced in tweet.get(\"referenced_tweets\", {})]),\n",
    "            \"reference_types\": \", \".join([referenced.get(\"type\") for referenced in tweet.get(\"referenced_tweets\", {})]),\n",
    "            \"text\": tweet[\"text\"].replace(\"\\n\", \" \")\n",
    "        }\n",
    "        writer.writerow(tweet_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c52003",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## trafilatura\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://europa.eu/citizens-initiative-forum/blog/europeans-safe-connections-call-stronger-regulation-wireless-internet-schools_en\"\n",
    "downloaded = trafilatura.fetch_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trafilatura.extract(downloaded)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa042243",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trafilatura.extract(\n",
    "    downloaded,\n",
    "    output_format=\"xml\",\n",
    "    url=url,\n",
    "    #include_comments=True,\n",
    "    #include_formatting=True,\n",
    "    #include_links=True,\n",
    "    #include_images=True,\n",
    "    #include_tables=True,\n",
    "    #favor_precision=True,\n",
    "    #favor_recall=True,\n",
    "    #target_language=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minidom.parseString(result).toprettyxml(indent=\"    \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa73b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trafilatura.extract(downloaded, output_format=\"csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f236fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trafilatura.extract(downloaded, output_format=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45041a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura.spider import focused_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "homepage = \"https://europa.eu/citizens-initiative-forum/blog_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78506af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a crawl\n",
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(to_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(known_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resuming a crawl\n",
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the crawl links\n",
    "sorted([url for url in known_urls if url.startswith(\"https://europa.eu/citizens-initiative-forum/blog/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bdca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bdb9dd",
   "metadata": {},
   "source": [
    "## Python basics\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basis: variable assignment\n",
    "corpus = \"This is a very tiny corpus. But at least this tiny corpus has a second sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef160a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content of a variable\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in jupyter notebooks that's enough to print a variable - if it's the last row of a cell\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of any Python object (everything is an object in Python)\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec00201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects / data types have their own methods\n",
    "tokens = corpus.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split() returns a list (as indicated by the square brackets)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable/sequential data types support selection and operations based on index positions\n",
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations can be nested and/or concatenated\n",
    "[tokens[0], type(tokens[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings are iterable too\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the \"replace\" method to improve our tokenization on whitespace\n",
    "corpus = corpus.replace(\".\", \" .\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the punctuation gets splitted correctly\n",
    "tokens = corpus.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the built-in method len() calculates the length of iterable data types\n",
    "token_count = len(tokens)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b721e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_count is an integer object\n",
    "type(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86376e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know that our corpus consists of 18 tokens and 82 characters\n",
    "char_count = len(corpus)\n",
    "char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e70014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a for-loop we can access every item in a list or in other iterable data types\n",
    "for token in tokens:\n",
    "    print(\"token:\\t\" + token) # strings can be concatenated\n",
    "    #print(f\"token:\\t{token}\") # the f-string syntax does the trick as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe302a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the sentences of our corpus\n",
    "sentences = corpus.split(\".\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have two tasks: get rid of the empty string and\n",
    "# append the punctuation back to the sentences\n",
    "sentences_stripped = []\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    # with conditions we can check if a statement is true or false\n",
    "    if not sentence == \"\":\n",
    "        sentence += \" .\"\n",
    "        sentences_stripped.append(sentence)\n",
    "\n",
    "sentences_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29031b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts provide a mapping between keys and values\n",
    "token_lengths = {}\n",
    "type(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79dd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can assign values to keys with dict_variable[key] = value\n",
    "for token in tokens:\n",
    "    token_lengths[token] = len(token)\n",
    "\n",
    "token_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a simple counter for the token frequencies of our corpus\n",
    "counter = {}\n",
    "for token in tokens:\n",
    "    # check if we have already seen this token\n",
    "    if token in counter:\n",
    "        counter[token] += 1\n",
    "    # if not (we see this token for the first time):\n",
    "    else:\n",
    "        counter[token] = 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# often programming tasks can be solved in more than one way\n",
    "counter = {}\n",
    "for token in tokens:\n",
    "    if not token in counter:\n",
    "        counter[token] = 0\n",
    "    counter[token] += 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for many problems code already exists, which can be imported\n",
    "# from built-in or external Python modules\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# well, that's shorter...\n",
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40803960",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the most_common() method of Counter\n",
    "# to sort our token frequencies\n",
    "Counter(tokens).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08af4f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## spaCy\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a110f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to import the previously installed library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02050482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a tiny corpus\n",
    "spacy_corpus = \"Mary Lou McDonald grew up in a republican household in Dublin to a backdrop of the Troubles in Northern Ireland. \\\"My family's connections with the IRA would have been in the 1920s,\\\" she says.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beefc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# befor spaCy can annotate anything, we have to load a trained statistical model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f36586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp() runs the default model annotation pipeline on our example corpus\n",
    "# 'doc' now contains the annotated spaCy data object\n",
    "doc = nlp(spacy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the tokens of our corpus document\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy docs are composed of Token objects\n",
    "print(type(doc[0]))\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23780e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token objects have different attributes containing their linguistic annotations\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    print(token.pos_)\n",
    "    print(token.pos_, token.tag_)\n",
    "    print(token.lemma_)\n",
    "    print(token.morph)\n",
    "    print(token.ent_iob_, token.ent_type_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09080ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b21ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88114017",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<text>\\n\"\n",
    "for sentence in doc.sents:\n",
    "    vrt += \"<s>\\n\"\n",
    "    entity = False\n",
    "    for token in sentence:\n",
    "        if entity == False and token.ent_iob_ == \"B\":\n",
    "            entity = True\n",
    "            vrt += f\"<entity type=\\\"{token.ent_type_}\\\">\\n\"\n",
    "        elif entity == True and not token.ent_iob_ == \"I\":\n",
    "            entity = False\n",
    "            vrt += \"</entity type>\\n\"\n",
    "        vrt += \"\\t\".join((token.text, token.pos_, token.lemma_, str(token.morph), token.ent_type_, token.ent_iob_)) + \"\\n\"\n",
    "    vrt += \"</s>\\n\"\n",
    "vrt += \"</text>\\n\"\n",
    "\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921254f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<text>\\n\"\n",
    "for sentence in doc.sents:\n",
    "    vrt += \"<sentence>\\n\"\n",
    "    for token in sentence:\n",
    "        vrt += token.text + \"\\n\"\n",
    "    vrt += \"</sentence>\\n\"\n",
    "vrt += \"</text>\\n\"\n",
    "\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrt = \"<person>\\nMary\\nLou\\nMcDonald\\n</person>\"\n",
    "print(vrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb873384",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## data formats\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open(\"tweets.jsonl\", encoding=\"utf8\") as jsonl:\n",
    "    for tweet in jsonl:\n",
    "        tweets.append(json.loads(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da88315",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets.csv\", \"w\", encoding=\"utf8\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"author_id\", \"conversation_id\", \"created_at\", \"id\", \"in_reply_to_user_id\", \"like_count\", \"quote_count\", \"reply_count\", \"retweet_count\", \"referenced_tweet_ids\", \"reference_types\", \"text\"]\n",
    "    writer = csv.DictWriter(csvfile, dialect=\"excel\", delimiter=\";\", fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for tweet in tweets:        \n",
    "        tweet_fields = {\n",
    "            \"author_id\": tweet[\"author_id\"],\n",
    "            \"conversation_id\": tweet[\"conversation_id\"],\n",
    "            \"created_at\": tweet[\"created_at\"],\n",
    "            \"id\": tweet[\"id\"],\n",
    "            \"in_reply_to_user_id\": tweet.get(\"in_reply_to_user_id\"),\n",
    "            \"like_count\": tweet.get(\"public_metrics\", {}).get(\"like_count\"),\n",
    "            \"quote_count\": tweet.get(\"public_metrics\", {}).get(\"quote_count\"),\n",
    "            \"reply_count\": tweet.get(\"public_metrics\", {}).get(\"reply_count\"),\n",
    "            \"retweet_count\": tweet.get(\"public_metrics\", {}).get(\"retweet_count\"),\n",
    "            \"referenced_tweet_ids\": \", \".join([referenced.get(\"id\") for referenced in tweet.get(\"referenced_tweets\", {})]),\n",
    "            \"reference_types\": \", \".join([referenced.get(\"type\") for referenced in tweet.get(\"referenced_tweets\", {})]),\n",
    "            \"text\": tweet[\"text\"].replace(\"\\n\", \" \")\n",
    "        }\n",
    "        writer.writerow(tweet_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9c777",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## trafilatura\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c167068",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://europa.eu/citizens-initiative-forum/blog/europeans-safe-connections-call-stronger-regulation-wireless-internet-schools_en\"\n",
    "downloaded = trafilatura.fetch_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cee2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trafilatura.extract(downloaded)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92869035",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trafilatura.extract(\n",
    "    downloaded,\n",
    "    output_format=\"xml\",\n",
    "    url=url,\n",
    "    #include_comments=True,\n",
    "    #include_formatting=True,\n",
    "    #include_links=True,\n",
    "    #include_images=True,\n",
    "    #include_tables=True,\n",
    "    #favor_precision=True,\n",
    "    #favor_recall=True,\n",
    "    #target_language=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb002c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minidom.parseString(result).toprettyxml(indent=\"    \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d11172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trafilatura.extract(downloaded, output_format=\"csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trafilatura.extract(downloaded, output_format=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura.spider import focused_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a407794",
   "metadata": {},
   "outputs": [],
   "source": [
    "homepage = \"https://europa.eu/citizens-initiative-forum/blog_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a crawl\n",
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70deaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(to_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af62572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(known_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e096297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resuming a crawl\n",
    "to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the crawl links\n",
    "sorted([url for url in known_urls if url.startswith(\"https://europa.eu/citizens-initiative-forum/blog/\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd211",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Data analysis and visualisation with Python\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import spacy\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Path(\"nexis_corpus\").rglob(\"*.xml\")\n",
    "\n",
    "articles = []\n",
    "for newspaper in corpus:\n",
    "    xml_tree = ET.parse(str(newspaper))\n",
    "    doc_elements = xml_tree.findall(\"document\")\n",
    "    \n",
    "    for doc in doc_elements:\n",
    "        articles.append(\n",
    "            {\n",
    "                \"newspaper\": doc.find(\"metadata/source\").text,\n",
    "                \"publication_type\": doc.find(\"metadata/publication_type\").text,\n",
    "                \"distribution\": doc.find(\"metadata/distribution\").text,\n",
    "                \"year\": doc.find(\"metadata/year\").text,\n",
    "                \"text\": \" \".join([paragraph.strip() for paragraph in doc.find(\"text\").itertext()]).strip(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "articles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [article[\"text\"] for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "for article, doc in zip(articles, docs):\n",
    "    article[\"tagged\"] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for doc in docs for token in doc if not token.is_space]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in tokens]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(lemmas).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01082ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_sorted = []\n",
    "counts = []\n",
    "rank = []\n",
    "for i, (lemma, count) in enumerate(counter, 1):\n",
    "    lemmas_sorted.append(lemma)\n",
    "    counts.append(count)\n",
    "    rank.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d58402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=go.Scatter(\n",
    "        x=lemmas_sorted[:50],\n",
    "        y=counts[:50],\n",
    "        text=rank,\n",
    "        hovertemplate = \"<i>lemma</i>: <b>%{x}</b><br><i>count</i>: %{y}<br><i>rank</i>: %{text}<extra></extra>\",\n",
    "        #line_shape=\"spline\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Zipf distribution\",\n",
    "    xaxis_title=\"Lemma (lemmatized by spaCy)\",\n",
    "    yaxis_title=\"Occurences in corpus\",\n",
    "    template=\"simple_white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Scatter chart\", \"Bar chart\"),\n",
    "    x_title=\"Lemma (lemmatized by spaCy)\",\n",
    "    y_title=\"Occurences in corpus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lemmas_sorted[:30],\n",
    "        y=counts[:30],\n",
    "        text=rank,\n",
    "        hovertemplate = \"<i>lemma</i>: <b>%{x}</b><br><i>count</i>: %{y}<br><i>rank</i>: %{text}<extra></extra>\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=lemmas_sorted[:30],\n",
    "        y=counts[:30],\n",
    "        text=rank,\n",
    "        hovertemplate = \"<i>lemma</i>: <b>%{x}</b><br><i>count</i>: %{y}<br><i>rank</i>: %{text}<extra></extra>\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Zipf distribution\",\n",
    "    template=\"seaborn\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.layout.annotations[0][\"yshift\"] = 10\n",
    "fig.layout.annotations[1][\"yshift\"] = 10\n",
    "fig.layout.annotations[2][\"yshift\"] = -50\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_distribution = {\n",
    "    \"Sunday Mail\": {\"texts\": 0, \"tokens\": 0},\n",
    "    \"The Guardian\": {\"texts\": 0, \"tokens\": 0},\n",
    "    \"The Times\": {\"texts\": 0, \"tokens\": 0},\n",
    "}\n",
    "\n",
    "for article in articles:\n",
    "    newspaper = article[\"newspaper\"]\n",
    "    text_token_distribution[newspaper][\"texts\"] += 1\n",
    "    text_token_distribution[newspaper][\"tokens\"] += len(article[\"tagged\"])\n",
    "\n",
    "labels, texts, tokens = zip(*[(newspaper, values[\"texts\"], values[\"tokens\"]) for newspaper, values in text_token_distribution.items()])\n",
    "\n",
    "text_token_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4983a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"lightsalmon\", \"lightsteelblue\", \"gold\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    specs=[[{\"type\" :\"domain\"}, {\"type\": \"domain\"}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=labels,\n",
    "        values=texts,\n",
    "        direction=\"counterclockwise\",\n",
    "        rotation=0,\n",
    "        sort=True,\n",
    "        hole=.3\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=labels,\n",
    "        values=tokens,\n",
    "        direction=\"counterclockwise\",\n",
    "        rotation=0,\n",
    "        sort=True,\n",
    "        hole=.3\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textinfo=\"label+percent\",\n",
    "    textfont_size=15,\n",
    "    hovertemplate=\"<b>%{value}</b><extra></extra>\",\n",
    "    marker=dict(colors=colors, line=dict(color='#000000', width=2.3))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Text and token distribution - Pie chart\",\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    annotations=[\n",
    "        dict(text=\"Texts\", x=0.195, y=0.5, font_size=17, showarrow=False),\n",
    "        dict(text=\"Tokens\", x=0.815, y=0.5, font_size=17, showarrow=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            name=\"Texts\",\n",
    "            x=labels,\n",
    "            y=[t/sum(texts) for t in texts],\n",
    "            marker_color=\"steelblue\"\n",
    "        ),\n",
    "        go.Bar(\n",
    "            name=\"Tokens\",\n",
    "            x=labels,\n",
    "            y=[t/sum(tokens) for t in tokens],\n",
    "            marker_color=\"salmon\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker_line_color=\"rgb(8,48,107)\",\n",
    "    marker_line_width=1.5, opacity=0.6\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"group\",\n",
    "    bargroupgap=0.05,\n",
    "    title=\"Text and token distribution - Bar chart\",\n",
    "    xaxis_title=\"Newspaper\",\n",
    "    yaxis_title=\"Percentage\",\n",
    "    template=\"ggplot2\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31130bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_distribution = {\n",
    "    \"Sunday Mail\": {\"hits\": 0, \"in_texts\": 0, \"per_text\": []},\n",
    "    \"The Guardian\": {\"hits\": 0, \"in_texts\": 0, \"per_text\": []},\n",
    "    \"The Times\": {\"hits\": 0, \"in_texts\": 0, \"per_text\": []},\n",
    "}\n",
    "\n",
    "for article in articles:\n",
    "    tokens = [token.text for token in article[\"tagged\"]]\n",
    "    if \"London\" in tokens:\n",
    "        per_text = 0\n",
    "        for token in tokens:\n",
    "            if token == \"London\":\n",
    "                query_distribution[article[\"newspaper\"]][\"hits\"] += 1\n",
    "                per_text += 1\n",
    "        query_distribution[article[\"newspaper\"]][\"in_texts\"] += 1\n",
    "        query_distribution[article[\"newspaper\"]][\"per_text\"].append(per_text)\n",
    "\n",
    "print(query_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"lightsalmon\", \"lightsteelblue\", \"gold\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    specs=[[{\"type\" :\"domain\"}, {\"type\": \"domain\"}, {\"type\": \"domain\"}]],\n",
    "    subplot_titles=(\n",
    "        \"Absolute frequency\",\n",
    "        \"Relative frequency (instances per million words)\",\n",
    "        \"Dispersion (texts with at least one hit)\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=labels,\n",
    "        values=[query_distribution[label][\"hits\"] for label in labels],\n",
    "        direction=\"counterclockwise\",\n",
    "        rotation=0,\n",
    "        sort=True\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=labels,\n",
    "        values=[(query_distribution[label][\"hits\"]/text_token_distribution[label][\"tokens\"])*1000000 for label in labels],\n",
    "        direction=\"counterclockwise\",\n",
    "        rotation=0,\n",
    "        sort=True\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=labels,\n",
    "        values=[query_distribution[label][\"in_texts\"] for label in labels],\n",
    "        direction=\"counterclockwise\",\n",
    "        rotation=0,\n",
    "        sort=True\n",
    "    ),\n",
    "    row=1,\n",
    "    col=3\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textinfo=\"label+percent\",\n",
    "    textfont_size=15,\n",
    "    hovertemplate=\"<b>%{value}</b><extra></extra>\",\n",
    "    marker=dict(colors=colors, line=dict(color='#000000', width=2.3))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Results for query \\\"London\\\" - absolute vs. relative frequency vs. dispersion\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.layout.annotations[2][\"yshift\"] = -30\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"lightsalmon\", \"lightsteelblue\", \"gold\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in labels:\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=query_distribution[label][\"per_text\"],\n",
    "            name=label,\n",
    "            box_visible=True,\n",
    "            meanline_visible=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Results for query \\\"London\\\" - Dispersion (hits per text)\",\n",
    "    showlegend=False,\n",
    "    yaxis_title=\"No. hits per text\",\n",
    "    template=\"ggplot2\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b49ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
